{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- 각자 이 ipynb 파일의 **사본을 생성**하여 과제 Q0~Q3까지 채운 후 해당 파일을 깃허브에 업로드해주세요!"
      ],
      "metadata": {
        "id": "YTOK1lA7Hsz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN Sample Code in PyTorch"
      ],
      "metadata": {
        "id": "2msLr-G13YUo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 직접 읽어보며 돌려볼 수 있는 **쉬운** 예제 코드~\n",
        "- 코드 간단 설명:\n",
        "  - 길이 12짜리 binary sequence(0/1)를 입력으로 받아서, 시퀀스 안에 1-0-1 pattern이 한 번이라도 등장하면 1, 아니면 0을 맞추는 binary classification을 수행하는 RNN 분류기\n",
        "  - 마지막에는 demo sequence로 예측 + hidden state 변화까지 출력하는 프로그램"
      ],
      "metadata": {
        "id": "5JrL9Fl93d0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "EO04CkcR4bFR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 정답 label 만드는 함수\n",
        "- 역할:\n",
        "  - sequence(e.g., [1,0,1,0,0,...]) 안에 연속된 3칸이 1-0-1인 구간이 있는지 검사\n",
        "  - 있으면 label=1 / 없으면 label=0"
      ],
      "metadata": {
        "id": "TLXMSKiQ9k6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def has_101_pattern(seq):\n",
        "    for i in range(len(seq) - 2):\n",
        "        if seq[i] == 1 and seq[i+1] == 0 and seq[i+2] == 1:\n",
        "            return 1\n",
        "    return 0"
      ],
      "metadata": {
        "id": "rfrge_Gq8ZFx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 학습용 data 만드는 PatternDataset 클래스\n",
        "- 깨알 상식) PyTorch에서 Dataset은 \"데이터를 꺼내는 방식\"을 표준화한 클래스~\n",
        "  - 이걸 상속받아서 내가 하고자 하는 task에 부합하는 나만의 커스텀 Dataset 클래스를 만들어서 모델에 먹이는 겁니다 얍얍"
      ],
      "metadata": {
        "id": "OzaGLdAbGUSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset이 하는 일은 \"모델에 넣기 좋은 형태\"로 데이터를 제공하는 것!\n",
        "class PatternDataset(Dataset):\n",
        "    def __init__(self, n_samples=5000, seq_len=12):\n",
        "        self.data = []\n",
        "        # (입력 시퀀스, 정답 label)을 n_samples개만큼 저장\n",
        "        for _ in range(n_samples):\n",
        "            seq = [random.randint(0, 1) for _ in range(seq_len)]  # 1) seq_len 길이의 random sequence 생성(0/1)\n",
        "            label = has_101_pattern(seq)                          # 2) has_101_pattern(seq)로 정답 label 생성\n",
        "            self.data.append((seq, label))                        # 3) (seq, label)을 self.data에 저장\n",
        "\n",
        "    # Dataset 안에 sample이 몇 개인지 알려주는 매직 메소드!\n",
        "    # 밑의 두개의 메소드는 필수임!!!\n",
        "    # 이걸로 보통 DataLoader가 \"전체 크기\"를 알 수 있게 합니다\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    # idx번째 데이터를 꺼내서 pytorch tensor로 변환해주는 매직 메소드\n",
        "    def __getitem__(self, idx):\n",
        "        seq, label = self.data[idx]\n",
        "        x = torch.tensor(seq, dtype=torch.long)         # (T,) = (12,) / embedding은 정수 인덱스를 받기 때문에 dtype으로 long을 사용합니다!\n",
        "        y = torch.tensor(label, dtype=torch.float32)    # scalar / BCEWithLogitsLoss가 float label(0.0/1.0)을 기대하는 편이라 dtype으로 float32를 사용했어요\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "4legZWrO8YJp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- RNN 모델 클래스\n",
        "- pytorch에서 모델 클래스는 일반적으로 nn.Module을 상속해서 만들어요~"
      ],
      "metadata": {
        "id": "uTROYjWUINLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleRNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size=2, embed_dim=8, hidden_dim=16):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)  # 학습 연산을 위해 (0/1) -> '벡터'로 변환\n",
        "        self.rnn = nn.RNN(input_size=embed_dim, hidden_size=hidden_dim, batch_first=True) # sequence를 왼쪽부터 읽으면서 hidden state를 update\n",
        "        self.fc = nn.Linear(hidden_dim, 1) # 마지막 hidden state로 이진 분류 점수(logit) 출력\n",
        "\n",
        "    # 일반 학습/평가용 feedforward 순전파 함수\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        NOTE: B는 batch size, T는 시퀀스의 길이!\n",
        "\n",
        "        input x: (B, T) 0/1 token\n",
        "        return: logits (B,)\n",
        "        \"\"\"\n",
        "        emb = self.embed(x)            # (B, T, E) / 벡터화\n",
        "        out, h_n = self.rnn(emb)       # 각 시점의 hidden 기록인 out: (B, T, H) / 마지막 hidden state인 h_n: (1, B, H)\n",
        "        last_h = h_n[-1]               # (B, H) / 마지막 hidden만 추출\n",
        "        logits = self.fc(last_h)       # (B, H) -> (B, 1)\n",
        "        return logits.squeeze(1)       # (B,) / loss 계산 편하게 하기 위해 주로 이렇게 squeeze()라는 함수를 사용하여 모양을 맞춰줍니다\n",
        "\n",
        "    def forward_with_trace(self, x):\n",
        "        \"\"\"\n",
        "        시각화를 통해 이해할 수 있도록 time step별 hidden(out)과 마지막 예측(logits)을 함께 리턴하는 함수\n",
        "        x: (1, T) 단일 시퀀스만 넣는 것을 권장함\n",
        "        \"\"\"\n",
        "        emb = self.embed(x)            # (1, T, E)\n",
        "        out, h_n = self.rnn(emb)       # out: (1, T, H)\n",
        "        last_h = h_n[-1]               # (1, H)\n",
        "        logits = self.fc(last_h)       # (1, 1)\n",
        "        return logits.squeeze(1), out.squeeze(0)  # logits: (1,), out: (T, H)\n",
        "        # out.squeeze(0) 추가로 한 이유 : batch=1을 넣으면 shape이 (1,T,H)인데 이 batch의 차원(1)을 제거하여 (T,H)로 보기 좋게 만든것!\n",
        "        # (크게 중요한 건 아닌데 그냥 궁금하실까봐,,)"
      ],
      "metadata": {
        "id": "9B39dpKJ7Kl_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 메인 함수: train()\n",
        "- 내부 로직 STEP BY STEP 설명:\n",
        "  1. (train/val) dataset 생성\n",
        "  2. DataLoader로 배치 묶기\n",
        "  3. model / loss function / optimizer 준비\n",
        "  4. epoch 반복하며 train\n",
        "  5. epoch마다 검증(val) 정확도 출력\n",
        "  6. 마지막에 demo sequence 1개 넣어서 확률 출력\n",
        "  7. demo sequence에서 시간별 hidden state 일부 출력"
      ],
      "metadata": {
        "id": "OXV7srCf9Jhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    \"\"\"\n",
        "    <헷갈리는 개념 (코드 지피티 딸깍하지 말고 이젠 꼭 알아두자)>\n",
        "    - Dataset: 데이터 1개를 어떻게 꺼낼지 정의\n",
        "    - DataLoader: 여러 개를 묶어서 batch를 만들고, 섞고, 반복 가능한 형태로 제공\n",
        "    - shuffle : train은 섞어서 학습이 안정적이고 (True) / val은 평가하는 거니까 섞을 필요 없음 (False)\n",
        "    \"\"\"\n",
        "\n",
        "    train_ds = PatternDataset(n_samples=6000, seq_len=12)\n",
        "    val_ds   = PatternDataset(n_samples=1000, seq_len=12)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "    # model/loss function/optimizer 준비\n",
        "    model = SimpleRNNClassifier(embed_dim=8, hidden_dim=16).to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    # Epoch train loop\n",
        "    for epoch in range(1, 6):\n",
        "        model.train()                                   # dropout/batch regularization 등의 mode들이 train 모드로 바뀜\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for x, y in train_loader:                       # batch 단위로 (x, y) 받음\n",
        "            x, y = x.to(device), y.to(device)           # x:(B,T), y:(B,)\n",
        "\n",
        "            logits = model(x)                           # (B,) / forward 실행\n",
        "            loss = criterion(logits, y)                 # scalar값 (정답 y와 예측 점수인 logit을 비교하여 loss값 계산)\n",
        "\n",
        "            optimizer.zero_grad()                       # 이전 gradient를 0으로\n",
        "            loss.backward()                             # backpropagation\n",
        "            optimizer.step()                            # parameter update\n",
        "\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "\n",
        "        train_loss = total_loss / len(train_loader.dataset)\n",
        "\n",
        "        # validation\n",
        "        model.eval()                                    # 평가 모드\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():                           # 평가이므로 학습 때와 달리 gradient 계산 하지 않음\n",
        "            for x, y in val_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                logits = model(x)\n",
        "                probs = torch.sigmoid(logits)\n",
        "                preds = (probs >= 0.5).float()          # 0.5 이상이면 1로 예측하도록 구현\n",
        "                correct += (preds == y).sum().item()\n",
        "                total += y.numel()\n",
        "\n",
        "        val_acc = correct / total\n",
        "        print(f\"[Epoch {epoch}] train_loss={train_loss:.4f}  val_acc={val_acc:.4f}\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # DEMO: hidden state 흐름을 출력해보자!\n",
        "    # ----------------------------\n",
        "    model.eval()\n",
        "\n",
        "    demo_seq = [1,1,0,0,1,1,0,0,1,1,0,0] # 패턴 101이 있는 입력 시퀀스\n",
        "    demo = torch.tensor([demo_seq], dtype=torch.long).to(device)  # (1,T)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logit, h_trace = model.forward_with_trace(demo)\n",
        "        prob = torch.sigmoid(logit).item()\n",
        "\n",
        "    print(\"\\n=== Demo ===\")\n",
        "    print(\"Sequence:\", demo_seq)\n",
        "    print(\"Final prob(pattern=101):\", round(prob, 4))\n",
        "\n",
        "    # hidden trace 일부 출력(앞 3스텝 + 마지막 3스텝)\n",
        "    h_cpu = h_trace.cpu()  # (T,H)\n",
        "    print(\"\\nHidden state trace (show first 3 and last 3 time steps):\")\n",
        "    for t in list(range(3)) + list(range(len(demo_seq)-3, len(demo_seq))):\n",
        "        vec = h_cpu[t][:6].tolist()  # hidden_dim 16 중 앞 6개만 보기\n",
        "        vec = [round(v, 3) for v in vec]\n",
        "        print(f\"t={t:2d}, x_t={demo_seq[t]} -> h_t[:6]={vec}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAHlsjUj8Ntd",
        "outputId": "efaf895a-ed00-40bc-f0c0-597d76e32ca6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] train_loss=0.6073  val_acc=0.7580\n",
            "[Epoch 2] train_loss=0.5543  val_acc=0.7580\n",
            "[Epoch 3] train_loss=0.4919  val_acc=0.8000\n",
            "[Epoch 4] train_loss=0.3161  val_acc=0.9760\n",
            "[Epoch 5] train_loss=0.1579  val_acc=1.0000\n",
            "\n",
            "=== Demo ===\n",
            "Sequence: [1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0]\n",
            "Final prob(pattern=101): 0.3788\n",
            "\n",
            "Hidden state trace (show first 3 and last 3 time steps):\n",
            "t= 0, x_t=1 -> h_t[:6]=[-0.958, -0.915, -0.246, -0.268, -0.612, -0.899]\n",
            "t= 1, x_t=1 -> h_t[:6]=[-1.0, -0.703, 0.312, -0.334, -0.726, -0.964]\n",
            "t= 2, x_t=0 -> h_t[:6]=[-0.979, 0.784, 0.96, -0.75, 0.817, 0.782]\n",
            "t= 9, x_t=1 -> h_t[:6]=[-0.997, -0.553, 0.023, -0.702, -0.941, -0.932]\n",
            "t=10, x_t=0 -> h_t[:6]=[-0.983, 0.922, 0.978, -0.796, 0.683, 0.894]\n",
            "t=11, x_t=0 -> h_t[:6]=[0.838, -0.645, 0.332, -0.402, 0.994, 0.926]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 위 코드에서 demo_seq 변수를 아래 두 가지로 바꿔서 각각 실행해보세요~\n",
        "  - 패턴 있음: [1,0,1,0,0,0,0,0,0,0,0,0] → 확률 높아야 함\n",
        "  - 패턴 없음: [1,1,0,0,1,1,0,0,1,1,0,0] → 확률 낮아야 함"
      ],
      "metadata": {
        "id": "IkUeto3kjsbW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q0. 위 코드의 출력 결과 분석 & 두 가지 입력을 넣었을 때 각각의 결과를 비교 분석하시오."
      ],
      "metadata": {
        "id": "QBSysnq3kNjm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans)\n",
        "학습이 진행될수록 train loss는 감소하고 validation accuracy는 증가하여 안정적으로 패턴을 인식하고 있음.\n",
        "또한, hidden state trace에서도 시간이 지날수록 hidden state 값이 특정 방향으로 수렴하는 경향이 있음.\n",
        "패턴이 있을 경우, Final prob(최종 출력 확률) = 0.6622로 높고\n",
        "패턴이 없을 경우, Final prob(최종출력확률) = 0.9788로 낮음\n",
        "이는 입력 시퀀스에 목표 패턴이 포함되지 않았음을 모델이 구분할 수 있다는 것을 보여줌.\n",
        "\n"
      ],
      "metadata": {
        "id": "pifQ80I8kZr_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Sample code in PyTorch"
      ],
      "metadata": {
        "id": "njt9mJdlkiDQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 아래는 위와 동일하게 진행"
      ],
      "metadata": {
        "id": "G8lrNsxJFRGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "kW-QeXmXkhF9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def has_101_pattern(seq):\n",
        "    for i in range(len(seq) - 2):\n",
        "        if seq[i] == 1 and seq[i+1] == 0 and seq[i+2] == 1:\n",
        "            return 1\n",
        "    return 0"
      ],
      "metadata": {
        "id": "XMoSaivQFF7Q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatternDataset(Dataset):\n",
        "    def __init__(self, n_samples=5000, seq_len=12):\n",
        "        self.data = []\n",
        "        for _ in range(n_samples):\n",
        "            seq = [random.randint(0, 1) for _ in range(seq_len)]\n",
        "            label = has_101_pattern(seq)\n",
        "            self.data.append((seq, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq, label = self.data[idx]\n",
        "        x = torch.tensor(seq, dtype=torch.long)\n",
        "        y = torch.tensor([label], dtype=torch.float)\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "uo-gXIZOFF5Q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 여기서부터 LSTM 모델 클래스"
      ],
      "metadata": {
        "id": "0hzsnnU5FL2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleLSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size=2, embed_dim=8, hidden_dim=16):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)  # (0/1) -> 벡터로 변환\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embed_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embed(x)                         # (batch, seq_len, embed_dim)\n",
        "        out, (h_n, c_n) = self.lstm(emb)            # out: (batch, seq_len, hidden_dim)\n",
        "                                                    # h_n: (num_layers, batch, hidden_dim)\n",
        "                                                    # c_n: (num_layers, batch, hidden_dim) => lstm은 c_n이 추가됨.\n",
        "\n",
        "        last_h = h_n[-1]                            # (batch, hidden_dim)  마지막 layer의 마지막 hidden\n",
        "        logit = self.fc(last_h)                     # (batch, 1)\n",
        "        return logit, out, (h_n, c_n)\n"
      ],
      "metadata": {
        "id": "FyAeyB5sFF2b"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN과의 차이점??\n",
        "\n",
        "1. nn.RNN -> nn.LSTM\n",
        "2. lSTM은 hidden state(h) 말고도 cell state(c)가 추가되었다는 점\n",
        "3. forward 결과에 따른 형태? (output, (h_n, c_n))"
      ],
      "metadata": {
        "id": "OAsdRO6FFUe-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 메인 함수 : train()\n",
        "- 내부 로직 step by step 설명:\n",
        "  1. train, val dataset 생성\n",
        "  2. DataLoader로 배치 묶기\n",
        "  3. model, loss function, optimizer 준비\n",
        "  4. epoch 반복하며 train\n",
        "  5. epoch마다 검증 정확도 출력\n",
        "  6. 마지막에 demo seq 하나 넣어서 확률 출력\n",
        "  7. demo seq에서 시간별 hidden state 출력"
      ],
      "metadata": {
        "id": "A5EtTq-8FVOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    train_ds = PatternDataset(n_samples=6000, seq_len=12)\n",
        "    val_ds   = PatternDataset(n_samples=1000, seq_len=12)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "    model = SimpleLSTMClassifier(vocab_size=2, embed_dim=8, hidden_dim=16).to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()  # logit을 바로 넣는 BCE\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    n_epochs = 5\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        # ---- train ----\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)               # x:(B,12), y:(B,1)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logit, _, _ = model(x)                           # logit:(B,1)\n",
        "            loss = criterion(logit, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "\n",
        "        avg_loss = total_loss / len(train_ds)\n",
        "\n",
        "        # ---- val ----\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for x, y in val_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                logit, _, _ = model(x)\n",
        "                prob = torch.sigmoid(logit)                  # (B,1)\n",
        "                pred = (prob >= 0.5).float()                 # (B,1)\n",
        "                correct += (pred == y).sum().item()\n",
        "                total += y.numel()\n",
        "\n",
        "        acc = correct / total\n",
        "        print(f\"[Epoch {epoch:02d}] loss={avg_loss:.4f} | val_acc={acc:.4f}\")\n",
        "\n",
        "    # ---- demo ----\n",
        "    model.eval()\n",
        "\n",
        "    demo_seq = [1,0,1,0,0,0,1,1,0,0,0,0] # 패턴 없음 -> 확률 낮아야 함\n",
        "\n",
        "    x_demo = torch.tensor(demo_seq, dtype=torch.long).unsqueeze(0).to(device)  # (1, 12)\n",
        "    logit, out_all, (h_n, c_n) = model(x_demo)\n",
        "\n",
        "    prob = torch.sigmoid(logit).item()\n",
        "    print(\"\\n--- DEMO ---\")\n",
        "    print(\"demo_seq:\", demo_seq)\n",
        "    print(f\"pred_prob(pattern=1): {prob:.4f}\")\n",
        "\n",
        "    # 시간별 hidden state 일부 출력\n",
        "    # out_all: (1, seq_len, hidden_dim)  -> time step별 hidden이 들어있음 (LSTM의 output)\n",
        "    out_all = out_all.squeeze(0).detach().cpu()  # (seq_len, hidden_dim)\n",
        "\n",
        "    print(\"\\n[time step별 hidden state 앞 6개 차원만 출력]\")\n",
        "    for t in range(out_all.size(0)):\n",
        "        h_t = out_all[t, :6].numpy()\n",
        "        print(f\"t={t:02d}, x={demo_seq[t]} -> h_t[:6]={h_t}\")\n",
        "\n",
        "    # (참고) 마지막 hidden/cell state도 같이 보기\n",
        "    last_h = h_n[-1].squeeze(0).detach().cpu()    # (hidden_dim,)\n",
        "    last_c = c_n[-1].squeeze(0).detach().cpu()    # (hidden_dim,)\n",
        "    print(\"\\n[마지막 state 요약]\")\n",
        "    print(\"last_h[:6] =\", last_h[:6].numpy())\n",
        "    print(\"last_c[:6] =\", last_c[:6].numpy())\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "stLhRCNMFF0L"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "돌려돌려"
      ],
      "metadata": {
        "id": "1QxJeYuWFcOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSe_IZ-DFFyD",
        "outputId": "f56890f5-1091-4878-bf82-705a71875e29"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 01] loss=0.5946 | val_acc=0.7520\n",
            "[Epoch 02] loss=0.5402 | val_acc=0.7690\n",
            "[Epoch 03] loss=0.5038 | val_acc=0.7960\n",
            "[Epoch 04] loss=0.4374 | val_acc=0.8630\n",
            "[Epoch 05] loss=0.2348 | val_acc=0.9900\n",
            "\n",
            "--- DEMO ---\n",
            "demo_seq: [1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0]\n",
            "pred_prob(pattern=1): 0.9856\n",
            "\n",
            "[time step별 hidden state 앞 6개 차원만 출력]\n",
            "t=00, x=1 -> h_t[:6]=[-0.14927247  0.10760365 -0.57531685  0.03524328 -0.08228742  0.03577602]\n",
            "t=01, x=0 -> h_t[:6]=[0.12429875 0.4549031  0.5067772  0.42054597 0.5211058  0.45365185]\n",
            "t=02, x=1 -> h_t[:6]=[-0.13893569  0.41794366 -0.63600016  0.04812949 -0.49096635  0.52203757]\n",
            "t=03, x=0 -> h_t[:6]=[ 0.39560062  0.6936992   0.611209    0.12222692 -0.2553333   0.7845735 ]\n",
            "t=04, x=0 -> h_t[:6]=[ 0.52751034  0.33455274  0.711323   -0.08973891 -0.6937065   0.8415857 ]\n",
            "t=05, x=0 -> h_t[:6]=[ 0.612663    0.22101209  0.7162833  -0.19474007 -0.8612494   0.8861636 ]\n",
            "t=06, x=1 -> h_t[:6]=[ 0.7236881   0.4196723  -0.41104895 -0.0616638  -0.86753696  0.93886435]\n",
            "t=07, x=1 -> h_t[:6]=[ 0.8479242   0.60941017  0.0798171  -0.06550428 -0.8655347   0.9741617 ]\n",
            "t=08, x=0 -> h_t[:6]=[ 0.86230034  0.76113224  0.7035788  -0.23194893 -0.9480296   0.97434247]\n",
            "t=09, x=0 -> h_t[:6]=[ 0.8897135   0.7072645   0.71748453 -0.23309687 -0.9726785   0.97272474]\n",
            "t=10, x=0 -> h_t[:6]=[ 0.90158606  0.6769155   0.7178277  -0.2376866  -0.979046    0.97369605]\n",
            "t=11, x=0 -> h_t[:6]=[ 0.90720975  0.65809536  0.7170575  -0.23832336 -0.9808351   0.97414494]\n",
            "\n",
            "[마지막 state 요약]\n",
            "last_h[:6] = [ 0.90720975  0.65809536  0.7170575  -0.23832336 -0.9808351   0.97414494]\n",
            "last_c[:6] = [ 2.3157668   0.8134583   0.9260874  -0.37662426 -3.7342405   3.158919  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- RNN과 비교할 점 :\n",
        "  1. val_acc\n",
        "  2. train_loss\n",
        "  3. demo prob\n",
        "\n",
        "\n",
        "- RNN vs LSTM\n",
        "\n",
        "  현재는 장기기억이 필요 없어서 유사한 상황.\n",
        "  \n",
        "  trade-off 중요성"
      ],
      "metadata": {
        "id": "0XZsF577Fgy1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRU Sample code in PyTorch"
      ],
      "metadata": {
        "id": "XRjTLGz8FkJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "FZQRzd_yFFwH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def has_101_pattern(seq):\n",
        "    for i in range(len(seq) - 2):\n",
        "        if seq[i] == 1 and seq[i+1] == 0 and seq[i+2] == 1:\n",
        "            return 1\n",
        "    return 0"
      ],
      "metadata": {
        "id": "IyxVJD_HFFtX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatternDataset(Dataset):\n",
        "    def __init__(self, n_samples=5000, seq_len=12):\n",
        "        self.data = []\n",
        "        for _ in range(n_samples):\n",
        "            seq = [random.randint(0, 1) for _ in range(seq_len)]\n",
        "            label = has_101_pattern(seq)\n",
        "            self.data.append((seq, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq, label = self.data[idx]\n",
        "        x = torch.tensor(seq, dtype=torch.long)\n",
        "        y = torch.tensor([label], dtype=torch.float)\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "Py8J1mxyFFq3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- GRU 클래스:\n",
        "  nn.GRU\n",
        "  \n",
        "  LSTM처럼 cell state(c)가 없고, hidden state(h) 하나만 유지\n",
        "\n",
        "  forward 결과로 out, h_n\n",
        "  \n",
        "  out : 모든 time step의 hidden (batch, seq_len, hidden_dim)\n",
        "\n",
        "  h_n : 마지막 hidden (num_layers, batch, hidden_dim)"
      ],
      "metadata": {
        "id": "tXUAnZxhFtM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleGRUClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size=2, embed_dim=8, hidden_dim=16):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=embed_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embed(x)                 # (B, T, E)\n",
        "        out, h_n = self.gru(emb)            # out: (B, T, H), h_n: (1, B, H) => c_n 없음.\n",
        "        last_h = h_n[-1]                    # (B, H)\n",
        "        logit = self.fc(last_h)             # (B, 1)\n",
        "        return logit, out, h_n"
      ],
      "metadata": {
        "id": "n_7qciOPFFoo"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gru():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    train_ds = PatternDataset(n_samples=6000, seq_len=12)\n",
        "    val_ds   = PatternDataset(n_samples=1000, seq_len=12)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "    model = SimpleGRUClassifier(vocab_size=2, embed_dim=8, hidden_dim=16).to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    n_epochs = 5\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        # ---- train ----\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logit, _, _ = model(x)\n",
        "            loss = criterion(logit, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "\n",
        "        avg_loss = total_loss / len(train_ds)\n",
        "\n",
        "        # ---- val ----\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for x, y in val_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                logit, _, _ = model(x)\n",
        "                prob = torch.sigmoid(logit)\n",
        "                pred = (prob >= 0.5).float()\n",
        "                correct += (pred == y).sum().item()\n",
        "                total += y.numel()\n",
        "\n",
        "        acc = correct / total\n",
        "        print(f\"[Epoch {epoch}] train_loss={avg_loss:.4f}  val_acc={acc:.4f}\")\n",
        "\n",
        "    # ---- demo ----\n",
        "    model.eval()\n",
        "\n",
        "    demo_seq = [1,0,1,0,0,0,1,1,0,0,0,0]  # 패턴 있음\n",
        "    x_demo = torch.tensor(demo_seq, dtype=torch.long).unsqueeze(0).to(device)  # (1, 12)\n",
        "\n",
        "    logit, out_all, h_n = model(x_demo)\n",
        "    prob = torch.sigmoid(logit).item()\n",
        "\n",
        "    print(\"\\n=== Demo ===\")\n",
        "    print(\"Sequence:\", demo_seq)\n",
        "    print(f\"Final prob(pattern=101): {prob:.4f}\")\n",
        "\n",
        "    # time step별 hidden state 출력 (앞 6개 차원)\n",
        "    out_all = out_all.squeeze(0).detach().cpu()  # (T, H)\n",
        "\n",
        "    print(\"\\nHidden state trace (show first 3 and last 3 time steps):\")\n",
        "    T = out_all.size(0)\n",
        "    for t in list(range(3)) + list(range(T-3, T)):\n",
        "        h_t = out_all[t, :6].numpy()\n",
        "        # 보기 좋게 소수점 3자리로\n",
        "        h_t_fmt = [float(f\"{v:.3f}\") for v in h_t]\n",
        "        print(f\"t={t:2d}, x_t={demo_seq[t]} -> h_t[:6]={h_t_fmt}\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "nrZXy-8iFFlr"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gru_model = train_gru()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARHGR8JkFFjY",
        "outputId": "21647012-047a-450d-80f7-a7a384d9b5ce"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] train_loss=0.5862  val_acc=0.7040\n",
            "[Epoch 2] train_loss=0.5113  val_acc=0.7580\n",
            "[Epoch 3] train_loss=0.3235  val_acc=0.9710\n",
            "[Epoch 4] train_loss=0.0772  val_acc=1.0000\n",
            "[Epoch 5] train_loss=0.0313  val_acc=1.0000\n",
            "\n",
            "=== Demo ===\n",
            "Sequence: [1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0]\n",
            "Final prob(pattern=101): 0.9962\n",
            "\n",
            "Hidden state trace (show first 3 and last 3 time steps):\n",
            "t= 0, x_t=1 -> h_t[:6]=[0.008, 0.439, 0.239, -0.475, 0.13, -0.174]\n",
            "t= 1, x_t=0 -> h_t[:6]=[0.733, -0.082, -0.898, -0.621, -0.425, -0.45]\n",
            "t= 2, x_t=1 -> h_t[:6]=[0.659, 0.833, -0.784, -0.833, 0.794, -0.639]\n",
            "t= 9, x_t=0 -> h_t[:6]=[0.986, 0.969, -0.918, -0.994, 0.943, -0.964]\n",
            "t=10, x_t=0 -> h_t[:6]=[0.986, 0.967, -0.917, -0.994, 0.942, -0.962]\n",
            "t=11, x_t=0 -> h_t[:6]=[0.985, 0.965, -0.916, -0.994, 0.942, -0.962]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- LSTM vs GRU ?"
      ],
      "metadata": {
        "id": "lnW68WglF24v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM & GRU 과제"
      ],
      "metadata": {
        "id": "gmuBRGxgF5ne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델의 장기기억 성능을 비교하기 위한 코드입니다.\n",
        "\n",
        "코드 중간의 빈칸을 채우면서, 매애앤 아래의 답변을 채워주시면 됩니다.\n",
        "\n",
        "모르면 인공지능을 사용해도 좋지만, sample code로도 풀 수 있으니 최대한 본인의 힘으로 해보면 좋겠습니다 !!"
      ],
      "metadata": {
        "id": "zM42wRXOF8Tu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "bItDmp57FyrU"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tmi) 11/7은 제 생일입니다. 감사합니다.\n",
        "def set_seed(seed=117):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(117)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QkVj3qZuFyqA",
        "outputId": "324bfe38-0b53-47d9-cd6f-d90c3261090b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LongMemoryDataset(Dataset):\n",
        "\n",
        "    def __init__(self, n_samples=8000, T=80):\n",
        "        self.T = T\n",
        "        self.data = []\n",
        "        for _ in range(n_samples):\n",
        "            first_bit = random.randint(0, 1)          # 기억해야 할 정보\n",
        "            middle = [random.randint(0, 1) for _ in range(T-2)]\n",
        "            seq = [first_bit] + middle + [2]          # 마지막은 DELIM=2\n",
        "            label = first_bit\n",
        "            self.data.append((seq, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq, label = self.data[idx]\n",
        "        x = torch.tensor(seq, dtype=torch.long)               # (T,)\n",
        "        y = torch.tensor([label], dtype=torch.float)          # (1,)\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "hZrbG1IeFyoJ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. lstm 모델 빈칸 채우기"
      ],
      "metadata": {
        "id": "iLi1JgS9GFQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size=3, embed_dim=8, hidden_dim=32):\n",
        "        super().__init__()\n",
        "        # TODO : 임베딩 레이어를 선언하세요.\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        # TODO : LSTM 레이어를 선언하세요. (batch_first=True)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embed_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T)\n",
        "        # TODO: 임베딩을 통과시키세요.\n",
        "        emb = self.embed(x)\n",
        "        # TODO : LSTM에 넣고 out, (h_n, c_n)을 받으세요.\n",
        "        out, (h_n, c_n) = self.lstm(emb)\n",
        "        # TODO : 마지막 hidden(last_h)을 얻으세요.\n",
        "        last_h = h_n[-1]\n",
        "        logit = self.fc(last_h)\n",
        "        return logit, out, (h_n, c_n)"
      ],
      "metadata": {
        "id": "HWz4-GNaFymh"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. gru 모델 빈칸 채우기"
      ],
      "metadata": {
        "id": "U3dXxnYlGGih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size=3, embed_dim=8, hidden_dim=32):\n",
        "        super().__init__()\n",
        "        # TODO : 임베딩 레이어\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        # TODO : GRU 레이어 (batch_first=True)\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=embed_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T)\n",
        "        # TODO : 임베딩\n",
        "        emb = self.embed(x)\n",
        "        # TODO : GRU forward로 out, h_n 받기\n",
        "        out, h_n = self.gru(emb)\n",
        "        # TODO : 마지막 hidden\n",
        "        last_h = h_n[-1]\n",
        "        logit = self.fc(last_h)\n",
        "        return logit, out, h_n"
      ],
      "metadata": {
        "id": "SXYh1847Fyko"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습 루프 만들기"
      ],
      "metadata": {
        "id": "ogOE47LuGP7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, epochs=6, lr=1e-3, device=\"cpu\", tag=\"\"):\n",
        "    model = model.to(device)\n",
        "    # TODO : loss 함수 선언 (BCEWithLogitsLoss)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    # TODO : optimizer 선언 (Adam)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        n = 0\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            # TODO : gradient 초기화\n",
        "            optimizer.zero_grad()\n",
        "            out = model(x)\n",
        "            logit = out[0] if isinstance(out, (tuple, list)) else out\n",
        "\n",
        "            # TODO : loss 계산\n",
        "            loss = criterion(logit, y)\n",
        "            # TODO : backprop\n",
        "            loss.backward()\n",
        "            # TODO : optimizer step\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "            n += x.size(0)\n",
        "\n",
        "        train_loss = total_loss / n\n",
        "\n",
        "        # validation\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for x, y in val_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                out = model(x)\n",
        "                logit = out[0] if isinstance(out, (tuple, list)) else out\n",
        "\n",
        "                # TODO : prob = sigmoid(logit)\n",
        "                prob = torch.sigmoid(logit)\n",
        "                # TODO : pred = (prob >= 0.5)\n",
        "                pred = (prob >= 0.5).float()\n",
        "\n",
        "                correct += (pred == y).sum().item()\n",
        "                total += y.numel()\n",
        "\n",
        "        val_acc = correct / total\n",
        "        print(f\"{tag}[Epoch {ep}] train_loss={train_loss:.4f}  val_acc={val_acc:.4f}\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "ELMItPqMGRG2"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 그대로 실행하시면 됩니다.\n",
        "\n",
        "T = 300\n",
        "train_ds = LongMemoryDataset(n_samples=8000, T=T)\n",
        "val_ds   = LongMemoryDataset(n_samples=2000, T=T)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "lstm = LSTMClassifier(vocab_size=3, embed_dim=8, hidden_dim=32)\n",
        "gru  = GRUClassifier(vocab_size=3, embed_dim=8, hidden_dim=32)\n",
        "\n",
        "print(\"=== LSTM ===\")\n",
        "lstm = train_model(lstm, train_loader, val_loader, epochs=6, lr=1e-3, device=device, tag=\"LSTM \")\n",
        "\n",
        "print(\"\\n=== GRU ===\")\n",
        "gru = train_model(gru, train_loader, val_loader, epochs=6, lr=1e-3, device=device, tag=\"GRU  \")"
      ],
      "metadata": {
        "id": "MqgsERicGRFS",
        "outputId": "c5d59bea-51a5-4388-9e11-657e559a53d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== LSTM ===\n",
            "LSTM [Epoch 1] train_loss=0.6933  val_acc=0.4910\n",
            "LSTM [Epoch 2] train_loss=0.6934  val_acc=0.5090\n",
            "LSTM [Epoch 3] train_loss=0.6932  val_acc=0.5090\n",
            "LSTM [Epoch 4] train_loss=0.6934  val_acc=0.4910\n",
            "LSTM [Epoch 5] train_loss=0.6933  val_acc=0.4970\n",
            "LSTM [Epoch 6] train_loss=0.6932  val_acc=0.4915\n",
            "\n",
            "=== GRU ===\n",
            "GRU  [Epoch 1] train_loss=0.6938  val_acc=0.5075\n",
            "GRU  [Epoch 2] train_loss=0.6937  val_acc=0.5090\n",
            "GRU  [Epoch 3] train_loss=0.6936  val_acc=0.4875\n",
            "GRU  [Epoch 4] train_loss=0.6936  val_acc=0.4910\n",
            "GRU  [Epoch 5] train_loss=0.6936  val_acc=0.5090\n",
            "GRU  [Epoch 6] train_loss=0.6933  val_acc=0.4970\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. LSTM과 GRU의 차이점에 대해서 간략하게 서술해주세요."
      ],
      "metadata": {
        "id": "M8o9PlFmGZMp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans) LSTM과 GRU는 gate 수에서 차이가 있음.\n",
        "LSTM과 GRU는 모두 RNN의 변형으로 장기 의존성 문제를 완화하기 위해 gate 구조를 도입한 모델임.\n",
        "LSTM은 입력 gate, 망각 gate, 출력 gate의 3개 게이트와 별도의 cell state를 사용하고\n",
        "GRU는 업데이트 gate와 리셋 gate의 2개 게이트만 사용하여 구조가 LSTM에 비해 더 단순함.\n",
        "이로 인해 GRU는 파라미터 수가 적고 학습 속도가 빠르고 LSTM은 더 복잡한 시계열 패턴을 안정적으로 기억하는 데 강점이 있음."
      ],
      "metadata": {
        "id": "SfE8A2EFGeHV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2. T=80에서 LSTM과 GRU의 학습 곡선을 비교하고, 어느 쪽이 더 안정적으로 수렴했는지 서술해주세요."
      ],
      "metadata": {
        "id": "PRfrs3zGGiDa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans) T=80 환경에서 두 모델 모두 안정적으로 학습되었지만,GRU가 더 큰 폭으로 loss를 감소시키며 수렴함.\n",
        "LSTM은 초기 train loss가 약 0.6936에서 0.6933으로 약 0.003 감소한 반면에 GRU는 0.6940에서 0.6935로 약 0.005 감소함.\n",
        "이는 GRU가 상대적으로 단순한 구조를 가지기 때문에 짧은 시퀀스 길이에서는 빠르고 안정적인 수렴을 보인 것으로 해석할 수 있음."
      ],
      "metadata": {
        "id": "IjAVyZcEGiBq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3. T를 80 → 150 → 300 순으로 늘려서 각각 실행해보고, 어떤 모델이 성능을 더 잘 유지하는지, 왜 그런 것 같은지를 서술해주세요."
      ],
      "metadata": {
        "id": "nWhHz27cGiAD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans) T가 증가할수록 LSTM이 GRU보다 성능을 더 잘 유지하는 경향이 있음.\n",
        "특히 T=300과 같이 긴 시퀀스 환경에서는 LSTM이 장기 의존 정보를 유지하는 능력이 더 뛰어나서\n",
        "학습 후반부에서도 비교적 안정적인 정확도를 보였다.\n",
        "이는 LSTM이 별도의 cell state와 망각 gate를 통해 중요한 정보를 장기간 보존할 수 있기 때문이라고 판단됨.\n",
        "반면 GRU는 구조가 단순한만큼 보다 긴 시퀀스에서는 정보 손실이 상대적으로 더 크게 발생한 것으로 판단됨."
      ],
      "metadata": {
        "id": "OpLgdVqOGh66"
      }
    }
  ]
}