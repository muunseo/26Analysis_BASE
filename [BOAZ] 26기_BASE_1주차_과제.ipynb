{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "WosjT1B5VeBj",
   "metadata": {
    "id": "WosjT1B5VeBj"
   },
   "source": [
    "# 1. ANN 과제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HtX76CHOYLCs",
   "metadata": {
    "id": "HtX76CHOYLCs"
   },
   "source": [
    "## Backpropagation의 가중치 업데이트\n",
    "\n",
    "다음은 17페이지에 제시된 **역전파(Backpropagation) 과정 중 가중치 업데이트 단계**를 나타낸 식이다.\n",
    "\n",
    "가중치 업데이트 단계는 앞선 단계에서 계산된 오차의 기울기를 이용해  \n",
    "손실 함수(Error)를 최소화하는 방향으로 가중치를 조정하는 과정이며,  \n",
    "이때 사용되는 규칙은 대표적인 **최적화 기법**에 해당한다.  \n",
    "\n",
    "\n",
    "$$\n",
    "W^{(l)} \\leftarrow W^{(l)} - \\alpha \\frac{\\partial E}{\\partial W^{(l)}}\n",
    "$$\n",
    "\n",
    "\n",
    "##  문제\n",
    "\n",
    "1. 위 가중치 업데이트 식이 의미하는 **최적화 기법**이 무엇인지 설명하시오.  \n",
    "2. 위 식에서  \n",
    "$$\n",
    "\\frac{\\partial E}{\\partial W^{(l)}}\n",
    "$$\n",
    "   가 어떻게 계산되는지를 **Chain Rule(연쇄법칙)** 을 이용해 설명하시오.  \n",
    "\n",
    "   (Hint: 세션 자료 하단에 제시된 최종 결과식을 참고헤 도출 과정 서술하면 됨)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "K7U-jSljepw-",
   "metadata": {
    "id": "K7U-jSljepw-"
   },
   "source": [
    "### 답변 1 : 경사하강법. 경사하강법은 함수의 기울기를 이용하여 그 기울기의 반대 방향으로 가중치를 이동시켜서 손실함수의 값을 최소하는 지점을 찾는 기법임.\n",
    "\n",
    "### 답변 2 : 가중치 w는 층의 선형 결과인 z에 영향을 주고 z는 최종 오차 E에 영향을 주기 때문에 연쇄법칙에 의해 분해할 수 있다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fksAbmqRnd7M",
   "metadata": {
    "id": "fksAbmqRnd7M"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GIgRo97NVut1",
   "metadata": {
    "id": "GIgRo97NVut1"
   },
   "source": [
    "# 2. DNN 과제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ny9dzza0f3kS",
   "metadata": {
    "id": "Ny9dzza0f3kS",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "다음은 세션 자료 25p의 과제의 내용이다. 자료에 주어진 조건을 바탕으로 아래 문제들을 해결하시오.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. 파라미터 수 직접 계산하기**\n",
    "주어진 신경망 구조(784 - 16 - 16 - 10)를 기준으로 총 학습 파라미터 수(가중치 $W$ 및 편향 $b$)를 직접 계산하시오.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ckt1B2wzgdlI",
   "metadata": {
    "id": "ckt1B2wzgdlI"
   },
   "source": [
    "### 답변 : 입력층 -> 첫번째 은닉층 : 784 * 16 + 1 * 16 / 첫번째 은닉층 -> 두번째 은닉층 : 16 * 16 + 1 * 16 / 두번째 은닉층 -> 출력층 : 16 * 10 + 10 ==> 총 파라미터 수 : 12560 + 272 +170 = 13002 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZAPWAdGFg81y",
   "metadata": {
    "id": "ZAPWAdGFg81y"
   },
   "source": [
    "---\n",
    "\n",
    "### **2. TensorFlow 모델 구현 및 검증**\n",
    "동일한 구조의 DNN을 TensorFlow/Keras 코드로 구현하고, `model.summary()` 출력 결과와 위에서 계산한 값이 일치하는지 확인하시오. (학습 과정은 필요 X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "RgagRLxIk6r1",
   "metadata": {
    "id": "RgagRLxIk6r1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-20 05:40:20.923888: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-20 05:40:20.953023: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-20 05:40:21.736614: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/home/jangmin-oh/.local/lib/python3.11/site-packages/keras/src/layers/core/dense.py:106: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1768887622.648047    5649 gpu_device.cc:2342] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,560</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">272</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">170</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │        \u001b[38;5;34m12,560\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m272\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m170\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,002</span> (50.79 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m13,002\u001b[0m (50.79 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,002</span> (50.79 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m13,002\u001b[0m (50.79 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 1. 적절한 라이브러리를 import하세요\n",
    "#!pip install tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "## 2. 모델 설계하기\n",
    "model = models.Sequential([\n",
    "    layers.Dense(16, activation='relu', input_shape =(784,)),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "## 3. 모델 summary 출력\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wofnlYRFl0jH",
   "metadata": {
    "id": "wofnlYRFl0jH"
   },
   "source": [
    "---\n",
    "\n",
    "### **3. 결과 비교**\n",
    "1번에서 직접 계산한 학습 파라미터 수와 model.summary() 출력 결과의 Total params 값이\n",
    "서로 일치하는지 확인하시오."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PJ-HtPs5mqwu",
   "metadata": {
    "id": "PJ-HtPs5mqwu"
   },
   "source": [
    "### 답변 : 1번에서 계산한 학습 파라미터 수는 13002이고 model.summary()로 확인한 결과 Total params = 13,002로 일치함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lnI4oyzXVu6h",
   "metadata": {
    "id": "lnI4oyzXVu6h"
   },
   "source": [
    "# 3. CNN 과제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96843df4",
   "metadata": {
    "id": "96843df4"
   },
   "source": [
    "---\n",
    "## **Introduction**\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:3744/format:webp/1*SGPGG7oeSvVlV5sOSQ2iZw.png)\n",
    "([Image Credit](https://medium.com/data-science/mnist-handwritten-digits-classification-using-a-convolutional-neural-network-cnn-af5fafbc35e9))\n",
    "\n",
    "Pytorch를 사용하여 이미지와 같이 MNIST 데이터셋을 분류하는 CNN 모델을 구현해봅시다.   \n",
    "Pytorch가 익숙하지 않은 분들은 [다음 튜토리얼](https://docs.pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)을 참고해주세요.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2b0f60",
   "metadata": {
    "id": "8c2b0f60"
   },
   "source": [
    "---\n",
    "## **1. Import Libraries & Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8e21858",
   "metadata": {
    "id": "a8e21858"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.8-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (52 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.61.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (114 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.9-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (11.0.0)\n",
      "Collecting pyparsing>=3 (from matplotlib)\n",
      "  Downloading pyparsing-3.3.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.8-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (355 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.61.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.9-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.3.1-py3-none-any.whl (121 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [fonttools]\u001b[33m  WARNING: The scripts fonttools, pyftmerge, pyftsubset and ttx are installed in '/home/jangmin-oh/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [matplotlib]6\u001b[0m [matplotlib]\n",
      "\u001b[1A\u001b[2KSuccessfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.61.1 kiwisolver-1.4.9 matplotlib-3.10.8 pyparsing-3.3.1\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리를 불러옵니다.\n",
    "!pip install matplotlib\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f26c1020",
   "metadata": {
    "id": "f26c1020"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9.91M/9.91M [00:02<00:00, 3.51MB/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28.9k/28.9k [00:00<00:00, 152kB/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.65M/1.65M [00:01<00:00, 1.22MB/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4.54k/4.54k [00:00<00:00, 2.07MB/s]\n"
     ]
    }
   ],
   "source": [
    "# 이미지 변환 함수를 정의합니다.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) ## 1. 이미지에 Normalize를 하는 이유와 2. 다음과 같은 숫자를 사용한 이유는 무엇일까요? (답은 작성하지 않으셔도 됩니다.)\n",
    "])\n",
    "\n",
    "# 데이터셋을 불러오고, DataLoader를 사용하여 데이터를 로드합니다.\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfebc3c1",
   "metadata": {
    "id": "bfebc3c1",
    "outputId": "daf29114-0a97-4720-c37d-9ec59589d5c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set size: 60000\n",
      "test set size: 10000\n",
      "\n",
      "training set dimension: torch.Size([60000, 28, 28])\n",
      "test set dimension: torch.Size([10000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "## 데이터셋의 크기와 차원을 확인합니다.\n",
    "print(f'training set size: {len(trainset)}')\n",
    "print(f'test set size: {len(testset)}\\n')\n",
    "\n",
    "print(f'training set dimension: {trainset.data.shape}')\n",
    "print(f'test set dimension: {testset.data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a787a4e2",
   "metadata": {
    "id": "a787a4e2"
   },
   "source": [
    "---\n",
    "## **2. Define a Convolutional Neural Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ebc72e",
   "metadata": {
    "id": "a8ebc72e"
   },
   "source": [
    "주어진 모델을 다시 한 번 정리해봅시다.\n",
    "\n",
    "\n",
    "**Conv_1** : 3x3 filter 32개, stride: 1, padding: 1, activation = 'relu'   \n",
    "**Pool_2** : 2x2 filter, stride: 2, padding: 0  \n",
    "**Conv_3** : 3x3 filter 64개, stride: 1, padding: 1, activation = 'relu'   \n",
    "**Pool_4** : 2x2 filter, stride: 2, padding: 0   \n",
    "**Dense_5** : 128, activation = 'relu'   \n",
    "**Dense_6** : 10, activation = 'softmax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1961c671",
   "metadata": {
    "id": "1961c671"
   },
   "outputs": [],
   "source": [
    "# 아래 코드의 빈칸을 채워주세요!\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(2, 2)\n",
    "        self.dense5 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.dense6 = nn.Linear(128,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool4(x)\n",
    "        x = torch.flatten(x, 1) # flatten layer: 2D -> 1D\n",
    "        x = F.relu(self.dense5(x))\n",
    "        x = self.dense6(x)\n",
    "        return x\n",
    "\n",
    "cnn = CNN()\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0735fb3a",
   "metadata": {
    "id": "0735fb3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dense5): Linear(in_features=3136, out_features=128, bias=True)\n",
      "  (dense6): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(2, 2)\n",
    "        self.dense5 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.dense6 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool4(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.dense5(x))\n",
    "        x = self.dense6(x)\n",
    "        return x\n",
    "\n",
    "cnn = CNN()\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2546f642",
   "metadata": {
    "id": "2546f642"
   },
   "source": [
    "#### **문제: 주어진 모델에 대해, layer마다 필요한 parameter의 수를 계산하세요.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e2dd6b",
   "metadata": {
    "id": "b2e2dd6b"
   },
   "source": [
    "- Conv_1: 320 => (필터높이 * 필터 너비 * 입력채널 + 편향) * 출력 채널\n",
    "- Pool_2: 0 => Pool은 파라미터가 없음. 단순히 최대값을 뽑아내는 연산이기 때문이다.\n",
    "- Conv_3: 18496\n",
    "- Pool_4: 0\n",
    "- FC_5: 401536 => Fully connected layer : (입력노드 수 * 출력노드 수) + 편향\n",
    "- FC_6: 1290"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf8b00a9",
   "metadata": {
    "id": "cf8b00a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchsummary in /home/jangmin-oh/.local/lib/python3.11/site-packages (1.5.1)\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 28, 28]             320\n",
      "         MaxPool2d-2           [-1, 32, 14, 14]               0\n",
      "            Conv2d-3           [-1, 64, 14, 14]          18,496\n",
      "         MaxPool2d-4             [-1, 64, 7, 7]               0\n",
      "            Linear-5                  [-1, 128]         401,536\n",
      "            Linear-6                   [-1, 10]           1,290\n",
      "================================================================\n",
      "Total params: 421,642\n",
      "Trainable params: 421,642\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.36\n",
      "Params size (MB): 1.61\n",
      "Estimated Total Size (MB): 1.97\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# summary를 사용하여 정답과 계산 결과가 일치하는지 확인하세요.\n",
    "!pip install torchsummary\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 모델을 해당 장치로 이동\n",
    "cnn = CNN().to(device)\n",
    "from torchsummary import summary\n",
    "\n",
    "summary(cnn, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc55961",
   "metadata": {
    "id": "fbc55961"
   },
   "source": [
    "---\n",
    "## **3. Train a model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9fce42e",
   "metadata": {
    "id": "d9fce42e"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "969c0825",
   "metadata": {
    "id": "969c0825"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# parameter gradient 초기화\u001b[39;00m\n\u001b[32m     12\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m outputs = \u001b[43mcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 모델을 통해 output 계산\u001b[39;00m\n\u001b[32m     15\u001b[39m loss = criterion(outputs, labels) \u001b[38;5;66;03m# loss 계산\u001b[39;00m\n\u001b[32m     16\u001b[39m loss.backward() \u001b[38;5;66;03m# gradient 계산 (backpropagation)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mCNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     x = F.relu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     13\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.pool2(x)\n\u001b[32m     14\u001b[39m     x = F.relu(\u001b[38;5;28mself\u001b[39m.conv3(x))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "\n",
    "    running_loss = 0\n",
    "    acc = 0\n",
    "\n",
    "    # training loop\n",
    "    cnn.train()\n",
    "    for i, data in enumerate(trainloader):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # parameter gradient 초기화\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = cnn(inputs) # 모델을 통해 output 계산\n",
    "        loss = criterion(outputs, labels) # loss 계산\n",
    "        loss.backward() # gradient 계산 (backpropagation)\n",
    "        optimizer.step() # parameter 업데이트\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print(f'[Epoch: {epoch + 1}, {i + 1:3d}] loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0\n",
    "\n",
    "    # testing loop\n",
    "    cnn.eval()\n",
    "    for i, data in enumerate(testloader):\n",
    "        inputs, labels = data\n",
    "        outputs = cnn(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        acc += (predicted == labels).sum().item()\n",
    "    acc = acc / len(testloader.dataset)\n",
    "    print(f\"====== Epoch {epoch + 1} Finished, Accuracy: {acc*100:.2f}% ======\")\n",
    "\n",
    "print(f\"\\nTraining finished. Final accuracy: {acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e92884",
   "metadata": {
    "id": "50e92884"
   },
   "source": [
    "---\n",
    "## **4. Experiment with the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69031ab",
   "metadata": {
    "id": "a69031ab"
   },
   "outputs": [],
   "source": [
    "# 기존의 모델을 원하는 대로 수정해보세요!\n",
    "# ex) layer 추가, kernel size 변경, optimizer 변경, activation 함수 변경, Dropout, etc.\n",
    "class newCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(newCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(2, 2)\n",
    "        self.conv_extra = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0)\n",
    "        self.pool_extra = nn.MaxPool2d(2, 2) \n",
    "        self.dense5 = nn.Linear(64 * 2 * 2, 128)\n",
    "        self.dense6 = nn.Linear(128,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool4(x)\n",
    "        x = F.relu(self.conv_extra(x))\n",
    "        x = self.pool_extra(x)\n",
    "        x = torch.flatten(x, 1) # flatten layer: 2D -> 1D\n",
    "        x = F.relu(self.dense5(x))\n",
    "        x = self.dense6(x)\n",
    "        return x\n",
    "\n",
    "new_cnn = newCNN()\n",
    "print(new_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b063f3",
   "metadata": {
    "id": "e7b063f3"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(new_cnn.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c143f9",
   "metadata": {
    "id": "e3c143f9"
   },
   "outputs": [],
   "source": [
    "for epoch in range(3):\n",
    "\n",
    "    running_loss = 0\n",
    "    acc = 0\n",
    "\n",
    "    # training loop\n",
    "    new_cnn.train()\n",
    "    for i, data in enumerate(trainloader):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # parameter gradient 초기화\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = new_cnn(inputs) # 모델을 통해 output 계산\n",
    "        loss = criterion(outputs, labels) # loss 계산\n",
    "        loss.backward() # gradient 계산 (backpropagation)\n",
    "        optimizer.step() # parameter 업데이트\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print(f'[Epoch: {epoch + 1}, {i + 1:3d}] loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0\n",
    "\n",
    "    # testing loop\n",
    "    new_cnn.eval()\n",
    "    for i, data in enumerate(testloader):\n",
    "        inputs, labels = data\n",
    "        outputs = new_cnn(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        acc += (predicted == labels).sum().item()\n",
    "    acc = acc / len(testloader.dataset)\n",
    "    print(f\"====== Epoch {epoch + 1} Finished, Accuracy: {acc*100:.2f}% ======\")\n",
    "\n",
    "print(f\"\\nTraining finished. Final accuracy: {acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b866aeb6",
   "metadata": {
    "id": "b866aeb6"
   },
   "source": [
    "### **기존의 모델에서 어떤 부분을 수정하였는지 설명해주세요.**\n",
    "\n",
    "답변 : 1. Conv_1의 커널은 5*5로 변경하였음 2. 한 개의 layer를 추가함. 3. Dense_5에서 데이터의 최종 크기가 작아져서 입력 노드가 줄어듦."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d3904f",
   "metadata": {
    "id": "75d3904f"
   },
   "source": [
    "### **이러한 변경 사항이 결과에 어떻게 영향을 미쳤나요?**\n",
    "\n",
    "답변 : 1. 커널 크기 확대 : 첫번째 계층의 커널 크기를 3*3에서 5*5로 확대하여 이미지의 더 넓은 지역적 특징을 초기에 더 잘 포착할 수 있음.\n",
    "2. 레이어 추가 : convolution 계층이 하나 더 추가되면서 모델이 복잡한 특징을 계층적으로 학습할 수 있게 되었음.\n",
    "3. 결국 추가된 레이어와 풀링 과정을 거치며 데이터의 최종 크기가 2*2로 작아짐/ 이로 인해 fully connected 계층으로 전달되는 입력 노드 수가 줄어들어 파라미터 수와 연산량이 감소하였음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeea8a0",
   "metadata": {
    "id": "eeeea8a0"
   },
   "source": [
    "---\n",
    "## **5. (정말 마지막) 이론 문제**\n",
    "convolution이 무엇인지, 그리고 이를 이용하면 이미지의 feature를 추출할 수 있음을 이해하기 위한 문제입니다.\n",
    "\n",
    "다음과 같은 input과 kernel이 존재할 때, 발표 자료의 방식과 같이 feature map을 구하고, 그 결과를 ?에 적으시오.\n",
    "![](https://i.imgur.com/v1wkvhW.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73830a73",
   "metadata": {
    "id": "73830a73"
   },
   "source": [
    "정답:   \n",
    "1 0   \n",
    "0 1"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
